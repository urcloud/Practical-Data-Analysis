{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbfHn_eiWuc9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lOdRIQU0_wE"
      },
      "source": [
        "Dataset $(x_k,y_k)$ generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4TMBIjixz6G"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "# 마찬가지로 난수를 예측 가능하도록 만들어주는 역할\n",
        "\n",
        "x = np.linspace(0, 4, 100)\n",
        "# 0에서 4까지 100개의 균등한 점 생성\n",
        "y = 3*np.sin(2*x) + np.random.normal(0, 0.2, size=x.shape)\n",
        "# 정규 분포 노이즈를 추가하여 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-LKPQO8x3Pd"
      },
      "outputs": [],
      "source": [
        "print(x[:5])\n",
        "print(y[:5])\n",
        "# x와 y의 첫 5개 값을 출력하여 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzpZwULZyyaR"
      },
      "source": [
        "Curve fit with linear function $f(x;\\beta)=\\beta_1*\\sin(\\beta_2*\\pi*x)$\n",
        "\n",
        "Define $E_2 = \\sum_{k=1}^{10}(y_k-f(x_k;\\beta))^2$\n",
        "\n",
        "Solving $A\\beta=y$ is impossible...\n",
        "\n",
        "Instead, let's consider $\\partial_{\\beta_i}E_2$ to utilize gradient descent:\n",
        "\n",
        "$\\beta_i^{n+1}=\\beta_i^{n+1}-\\delta\\partial_{\\beta_i}E_2(\\beta^{n})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sG0SQCxdpqH"
      },
      "outputs": [],
      "source": [
        "beta = np.array([1,1]) #initialize; beta^{1}\n",
        "# [1,1] 초기값 설정\n",
        "beta_list = [beta]\n",
        "# 최적화 과정에서의 𝛽값 리스트로 저장\n",
        "\n",
        "delta = 1e-3\n",
        "# 학습률(gradient descent의 step size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jplQ91sgDnh"
      },
      "outputs": [],
      "source": [
        "print(beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8rgjq-ueJOQ"
      },
      "source": [
        "Note that\n",
        "$\\partial_{i}E_2 = \\frac{1}{n_{sample}} 2*\\sum_{k=1}^{n_{sample}}(y_k-f(x_k;\\beta))(-\\partial_{i}f(x_k;\\beta))$\n",
        "hence,\n",
        "\n",
        "$\\partial_{1}E_2 = \\frac{1}{n_{sample}}2*\\sum_{k=1}^{n_{sample}}(y_k-f(x_k;\\beta))(-\\sin(\\beta_2*x_k))$\n",
        "\n",
        "$\\partial_{2}E_2 = \\frac{1}{n_{sample}}2*\\sum_{k=1}^{n_{sample}}(y_k-f(x_k;\\beta))(-\\beta_1\\cos(\\beta_2*x_k)*x_k)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0kDOY1keIgc"
      },
      "outputs": [],
      "source": [
        "def grad_E2(x,y,beta):\n",
        "  # E_2 함수에 대한 그래디언트(기울기) 계산하는 함수\n",
        "    grad_E2 = np.zeros(2)\n",
        "    # 기울기를 저장할 배열 초기화\n",
        "\n",
        "    if False: # \"Mini-Batch\" Gradient Descent if \"True\"\n",
        "      # 미니배치 경사 하강법을 활성화할 경우 일부 데이터만 선택하여 학습 가능\n",
        "      idx = np.random.randint(low=0, high=len(x), size= 10)\n",
        "      x_data = x[idx]\n",
        "      y_data = y[idx]\n",
        "    else:    # Classical Gradient Descent if \"False\"\n",
        "      # 현재는 전체 데이터를 사용 (클래식 경사 하강법)\n",
        "      x_data = x\n",
        "      y_data = y\n",
        "    common_term = y_data-beta[0]*np.sin(beta[1]*x_data)\n",
        "    # 모델의 예측값과 실제값 차이 계산\n",
        "    grad_E2[0] = 2*np.mean(common_term*(-np.sin(beta[1]*x_data)))\n",
        "    grad_E2[1] = 2*np.mean(common_term*(-beta[0]*np.cos(beta[1]*x_data)*x_data))\n",
        "    # β0 와 𝛽1에 대한 편미분을 각각 계산\n",
        "    return grad_E2\n",
        "  # 계산된 그래디언트를 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7cLfshVeIjX"
      },
      "outputs": [],
      "source": [
        "for k in range(10000):\n",
        "  beta = beta - delta*grad_E2(x,y,beta)\n",
        "  # 10,000번의 반복을 수행하며, 기울기 정보를 이용해 β를 업데이트\n",
        "\n",
        "  if k %100 == 0:\n",
        "    print(k,\"\\t\",*np.round(beta,3))\n",
        "    beta_list.append(list(beta))\n",
        "    # 100번마다 현재 β 값을 출력하고 beta_list에 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i1FVQUlXYnq"
      },
      "outputs": [],
      "source": [
        "def E2Loss(x,y,beta):\n",
        "  return np.sqrt(np.sum((y-beta[0]*np.sin(beta[1]*x))**2))\n",
        "# 손실 함수 E2를 정의\n",
        "# 예측값과 실제값의 차이를 제곱하고, 합한 뒤 루트를 취함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Y0J6WkXcaP"
      },
      "outputs": [],
      "source": [
        "beta0_range = np.linspace(-1, 4, 100)\n",
        "beta1_range = np.linspace(-1, 4, 100)\n",
        "# β0,β1의 범위를 -1에서 4까지 100개로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFHWcTMXx6Uv"
      },
      "outputs": [],
      "source": [
        "loss_values = np.zeros((len(beta0_range), len(beta1_range)))\n",
        "# 손실 값을 저장할 행렬 초기화\n",
        "\n",
        "for i, beta0 in enumerate(beta0_range):\n",
        "    for j, beta1 in enumerate(beta1_range):\n",
        "        loss_values[i, j] = E2Loss(x, y, [beta0, beta1])\n",
        "# 모든 𝛽0, 𝛽1 조합에 대해 손실 함수 값 계산\n",
        "\n",
        "B0, B1 = np.meshgrid(beta0_range, beta1_range)\n",
        "# β0,β1의 그리드(meshgrid) 생성 (3D 그래프용)\n",
        "\n",
        "# --- 3D Surface Plot ---\n",
        "fig = plt.figure(figsize=(12, 5))\n",
        "# 3D 플롯 생성\n",
        "\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.scatter(1, 1, E2Loss(x,y,[1,1]), color='red', s=10)\n",
        "# 초기 β=(1,1)의 손실값을 빨간 점으로 표시\n",
        "\n",
        "for k in range(len(beta_list)):\n",
        "  a_fit, b_fit = beta_list[k]\n",
        "  ax1.scatter(a_fit, b_fit, E2Loss(x,y,[a_fit,b_fit]), color='red', s=5)\n",
        "# 경사 하강법 과정에서 β의 변화를 빨간 점으로 시각화\n",
        "\n",
        "ax1.plot_surface(B0, B1, loss_values.T, cmap='viridis', edgecolor='none', alpha=0.5)\n",
        "# 손실 함수의 3D 표면 플롯\n",
        "\n",
        "ax1.set_xlabel(r'$\\beta_0$')\n",
        "ax1.set_ylabel(r'$\\beta_1$')\n",
        "ax1.set_zlabel('E2')\n",
        "ax1.set_title('Loss Landscape (3D Surface)')\n",
        "# 축 라벨과 제목 설정\n",
        "\n",
        "# --- Contour Plot ---\n",
        "ax2 = fig.add_subplot(122)\n",
        "contour = ax2.contourf(B0, B1, loss_values.T, levels=30, cmap='viridis')\n",
        "# 손실 함수를 등고선으로 표현\n",
        "fig.colorbar(contour, ax=ax2, label=\"Loss\")\n",
        "# 색상 막대 추가\n",
        "ax2.scatter(1, 1, color='red', s=10)\n",
        "for k in range(len(beta_list)):\n",
        "  a_fit, b_fit = beta_list[k]\n",
        "  ax2.scatter(a_fit, b_fit, color='red', s=5)\n",
        "  # 초기값과 학습 과정에서의 β 변화 표시\n",
        "  \n",
        "ax2.set_xlabel(r'$\\beta_0$')\n",
        "ax2.set_ylabel(r'$\\beta_1$')\n",
        "ax2.set_title('Loss Landscape (Contour Plot)')\n",
        "# 등고선 그래프 설정\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# 그래프 정렬 후 표시\n",
        "\n",
        "# 결론적으로 grad_E2를 이용해 경사 하강법으로 β를 최적화\n",
        "# 3D 및 등고선 그래프를 통해 손실 함수의 변화 및 최적화 경로 시각화"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
